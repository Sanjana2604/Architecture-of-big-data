# ABD Lab

[MapReduce in Textbook](https://www.notion.so/MapReduce-in-Textbook-f33d93da5635486688c158f6b013fca6)

### MapReduce - 1

- Data (text, integer) converted into key-value pair and given into map and reducer task
- Splits the input and convert into key-value pair, from all the datanodes, shuffles and then sorts again → reducer task → reducer generates the output, which is again in key-value format
- [0, package testPro; ]
- [18, import java. .... ;], 18 is the offset (key, value)
- Breaks the value input into tokens

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled.png)

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%201.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%201.png)

- The aggregation above, is done at the Mapper's level which was not done in previous versions of Hadoop
- Above is the result of Mappers present and sinimarly, each Mapper generates similar outputs
- Shuffler combines all the outputs of the Mappers and makes like [ package , <1,4, 7>
- key - package here, value is <1,4,7> (iterable element)
- Then, [package, 12] is done
- Word count

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%202.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%202.png)

- There are 3 words of 2 letters, there are 20 words which are 5 letter

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%203.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%203.png)

- Java → Jar file → run in hadoop environment
- Eclipse > New > Java Project > Name of the proejct (TestWordsCount)
- Use Default location
- Java SE - 1.8 (Use an execution environment JRE)
- Finish button
- Right click on src > New > Class
- Can create a package (optional ) else will be put under default package.
- Name : wordCount , check the PSVM method
- We require Hadoop jar files
- Go to TestWordCount - right click - Build path → Configure Build Path
- Add External JARs
- opt\hadoop\share\hadoop\common → hadoop-common-3.2.1.jar → Click open
- opt\hadoop\share\hadoop\mapreduce → hadoop-mapreduce-client-core-3.2.1.jar → Click open
- Apply → Apply and Close
- We can see in that in the Libraries → Referenced Libraries are added
- 1 → key LongWritable and value Text (Input)
- 2 → key as Text and IntWritable as value (Output)
- Context is to write the result of the Mapper. Also to store intermediate result

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%204.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%204.png)

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%205.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%205.png)

- Last two are the result <Text, IntWritable>

### Lab Session - 2 hours

```java
public class wordCount {
	
	public static class testWordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
					String line = value.toString();
					StringTokenizer tokenizer = new StringTokenizer (line);

					while (tokenizer.hasMoreTokens() ) {
						value.set(tokenizer.nextToken() );
						context.write(value, new IntWritable(1));
					}
		}
	}
//Data type is text -> comvert into String type and call the tokenizer
//By default it will be tokenized based on blank space
//for every word in the tokenizer iteration, it is assigned a value 1 by 
//IntWritable

	

	public static class testWordCountReducer extends Reducer <Text, IntWritable, Text, IntWritable > {
		public void reduce(Text key, Iterable<IntWritable> values, Context context) 
			throws IOException, InterruptedException {
				int sum = 0;
				for (IntWritable x: values) {
					sum += x.get();
				}
				context.write(key, new IntWritable(sum) );
		}
	}

//same input as the final output (for Reducer)
//Iterable list because we might get it like [package, <4,2,1>] (which
//is list of integers)
//context write it will write [package, 7]

	public static void main(String[] args) throws Exception{
		// TODO Auto-generated method stub
		Configuration conf = new Configuration();
		
		Job job = Job.getInstance(conf, "my count");
		//Configuration object and any name

		job.setJarByClass(wordCount.class);
		//classname and filename has to be same
		job.setMapperClass(testWordCountMapper.class);
		job.setReducerClass(testWordCountReducer.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);

		FileInputFormat.addInputPath(job, new Path(args[0]));
		
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		System.exit(job.waitForCompletion(true) ? 0: 1);
	}

}

//whatever is in main() we have to write it for all mapreduce program
```

- After writing this on the eclipse you might get a few errors, import whatever errors suggestion that it gives
- How to create JAR files
- Project > Export > JAR file (under JAVA) > home/sois/eclipse-workspace/wordcount.jar > Next > Finish
- No errors!
- How to run the jar file
- Go to command prompt

```markdown
>>jps
>>cd eclipse-workspace/
>>
// new command prompt
//create some text files
//when you create a new class

>>hadoop jar wordcount.jar wordCount /mr/inpur /mr/out
```

- This (below) is when I have imported a package. If accepted a default package then package name is not required

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%206.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%206.png)

- Else you will get a classnotfoundexception
- If it is not able to find Yarn classfile for the reducer

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%207.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%207.png)

- Go to cd /opt/hadoop/etc/hadoop

```markdown
>>ls
>>gedit mapred-site.xml

```

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%208.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%208.png)

- Make this change in the mapred-site.xml file

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%209.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%209.png)

- Removing the out folder present

![ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%2010.png](ABD%20Lab%201807a35869614e38af1f66e3110f46a6/Untitled%2010.png)

```markdown
>>hadoop jar wordcount.jar wordCount /mr/input /mr/out

>>hdfs dfs -ls /mr/out
```

- Can go to the webbrowser and see
- localhost:9870
- Utilities > Browse filesystem > mr> out > part-r-0000 > Head the file > Then you see the output

### PySpark

- RDD are RAM items which are immutable and can be subjected to transformation - action to produce a new RDD
- Transformations undergo lazy evaluations, and is performed as actions (actual executions takes place as actions)
- One of the reason why why RDD is immutable is because it keeps every transformations as checkpoints to begin from in case it is failed

```python

import pyspark
from pyspark import SparkSession
spark = SparkSession.builder.appName("Test RDD Examples").getOrCreate()

type(spark)

#Create RDDs
#1. Using parallelize method
rdd_par = spark.sparkContext.parallelize(["Hellowrld", "Juiceworld"])
#we leave the second param as it is

#creating RDD using transformations
rdd_trans = rdd_par.filter(lambda word:word.startsWith('H'))
rdd_trans.collect()

#creating RDD from a datasource
rdd_ds = spark.sparkContext.textfile('D:/input.txt')
#it still exectes cause lazy evaluation - only when I perform an action this is done
rdd_ds.count()
#throws an error if the file path is wrong

rdd_ds.flatMap(lambda word:word.split(' ')).collect()  #does a one to many transformation
#splits and produces all the words in the text file

rdd_ds.flatMap(lambda word:word.split(' ')).count()

freq_words = rdd_ds.map(lambda word:(word,1))
freq_words.reduceByKey(lambda a,b : a+b).collect() 
#values become a list for each word and then added
#reducing the (word, 1) and reduces it to (word, count)

```

- The parallelize tries to split the data to pass onto the executors present. If there's just one executor and the number of splits is specified, then different tasks goes into different cores of the single machine executor. If we don't have multiple cores, then tasks happens one after another